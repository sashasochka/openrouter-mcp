#!/usr/bin/env python3
"""
OpenRouter MCP Server ë²¤ì¹˜ë§ˆí‚¹ ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” OpenRouter MCP Serverì˜ ë²¤ì¹˜ë§ˆí‚¹ ê¸°ëŠ¥ì„ ì‹œì—°í•©ë‹ˆë‹¤.
ë‹¤ì–‘í•œ AI ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³  ë¶„ì„í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""

import asyncio
import os
import json
import logging
from datetime import datetime
from typing import List, Dict, Any

# ë²¤ì¹˜ë§ˆí‚¹ ê´€ë ¨ import
from src.openrouter_mcp.handlers.mcp_benchmark import (
    get_benchmark_handler,
    benchmark_models,
    get_benchmark_history,
    compare_model_categories,
    export_benchmark_report,
    compare_model_performance
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


async def example_1_basic_benchmarking():
    """ì˜ˆì œ 1: ê¸°ë³¸ ë²¤ì¹˜ë§ˆí‚¹"""
    print("ğŸš€ ì˜ˆì œ 1: ê¸°ë³¸ ë²¤ì¹˜ë§ˆí‚¹")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤ (ë¹ ë¥¸ ë¬´ë£Œ ëª¨ë¸ë“¤)
    models = [
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-haiku", 
        "google/gemini-flash-1.5",
        "meta-llama/llama-3.1-8b-instruct:free"
    ]
    
    prompt = "Pythonì—ì„œ 'Hello, World!'ë¥¼ ì¶œë ¥í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    
    try:
        result = await benchmark_models(
            models=models,
            prompt=prompt,
            runs=2,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 2íšŒë§Œ
            delay_seconds=0.5,
            save_results=True
        )
        
        print(f"âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ëœ ëª¨ë¸: {len(result['results'])}")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {result.get('summary', {}).get('total_time', 'N/A')}")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if 'ranking' in result:
            print("\nğŸ† ëª¨ë¸ ë­í‚¹:")
            for rank_info in result['ranking'][:3]:  # ìƒìœ„ 3ê°œë§Œ
                print(f"  {rank_info['rank']}. {rank_info['model_id']} "
                      f"(ì ìˆ˜: {rank_info['overall_score']:.2f})")
        
        return result.get('saved_file')
        
    except Exception as e:
        print(f"âŒ ê¸°ë³¸ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤íŒ¨: {e}")
        return None


async def example_2_category_comparison():
    """ì˜ˆì œ 2: ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë¹„êµ"""
    print("\nğŸ¯ ì˜ˆì œ 2: ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë¸ ë¹„êµ")
    print("=" * 50)
    
    try:
        result = await compare_model_categories(
            categories=["chat", "code"],
            top_n=2,
            metric="quality"
        )
        
        print(f"âœ… ì¹´í…Œê³ ë¦¬ ë¹„êµ ì™„ë£Œ!")
        print(f"ğŸ“Š ë¹„êµëœ ì¹´í…Œê³ ë¦¬: {result['config']['categories']}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´ ì¶œë ¥
        if 'category_info' in result:
            print("\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ì •ë³´:")
            for category, info in result['category_info'].items():
                print(f"  - {category}: {info['total_models']}ê°œ ëª¨ë¸ ì¤‘ "
                      f"{len(info['selected_models'])}ê°œ ì„ íƒ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ì¶œë ¥
        if 'results' in result:
            print("\nğŸ† ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸:")
            for category, models in result['results'].items():
                print(f"  {category.upper()}:")
                for model in models:
                    if model.get('success'):
                        metrics = model.get('metrics', {})
                        print(f"    - {model['model_id']}: "
                              f"í’ˆì§ˆ {metrics.get('quality_score', 0):.1f}, "
                              f"ë¹„ìš© ${metrics.get('avg_cost', 0):.6f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì¹´í…Œê³ ë¦¬ ë¹„êµ ì‹¤íŒ¨: {e}")
        return False


async def example_3_performance_analysis():
    """ì˜ˆì œ 3: ê³ ê¸‰ ì„±ëŠ¥ ë¶„ì„"""
    print("\nğŸ“ˆ ì˜ˆì œ 3: ê³ ê¸‰ ì„±ëŠ¥ ë¶„ì„")
    print("=" * 50)
    
    # ë¶„ì„í•  ëª¨ë¸ë“¤
    models = [
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-haiku"
    ]
    
    # ì‚¬ìš© ì‚¬ë¡€ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
    use_cases = {
        "speed_focused": {
            "speed": 0.5,
            "cost": 0.3,
            "quality": 0.2
        },
        "quality_focused": {
            "speed": 0.1,
            "cost": 0.2,
            "quality": 0.7
        },
        "balanced": {
            "speed": 0.25,
            "cost": 0.25,
            "quality": 0.25,
            "throughput": 0.25
        }
    }
    
    try:
        print("âš–ï¸ ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì¤‘...")
        
        for use_case, weights in use_cases.items():
            print(f"\nğŸ“‹ {use_case.replace('_', ' ').title()} ì‹œë‚˜ë¦¬ì˜¤:")
            
            result = await compare_model_performance(
                models=models,
                weights=weights,
                include_cost_analysis=True
            )
            
            if result.get('success'):
                ranking = result.get('ranking', [])
                print(f"  ğŸ† 1ìœ„: {ranking[0]['model_id']} (ì ìˆ˜: {ranking[0]['weighted_score']:.3f})")
                if len(ranking) > 1:
                    print(f"  ğŸ¥ˆ 2ìœ„: {ranking[1]['model_id']} (ì ìˆ˜: {ranking[1]['weighted_score']:.3f})")
                
                # ì¶”ì²œì‚¬í•­ ì¶œë ¥
                recommendations = result.get('recommendations', [])
                for rec in recommendations[:1]:  # ì²« ë²ˆì§¸ ì¶”ì²œë§Œ
                    print(f"  ğŸ’¡ {rec['type']}: {rec['model']} - {rec['reason']}")
            else:
                print(f"  âŒ {use_case} ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return False


async def example_4_benchmark_history():
    """ì˜ˆì œ 4: ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ ì¡°íšŒ"""
    print("\nğŸ“š ì˜ˆì œ 4: ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ ì¡°íšŒ")
    print("=" * 50)
    
    try:
        result = await get_benchmark_history(
            limit=5,
            days_back=7
        )
        
        history = result.get('history', [])
        
        if history:
            print(f"âœ… {len(history)}ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ ë°œê²¬")
            
            print("\nğŸ“‹ ìµœê·¼ ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡:")
            for i, record in enumerate(history, 1):
                print(f"  {i}. {record['filename']}")
                print(f"     - ì‹œê°„: {record['timestamp']}")
                print(f"     - ëª¨ë¸: {', '.join(record['models_tested'])}")
                print(f"     - ì„±ê³µë¥ : {record['success_rate']}")
                if record['summary'].get('avg_response_time'):
                    print(f"     - í‰ê·  ì‘ë‹µì‹œê°„: {record['summary']['avg_response_time']:.2f}ì´ˆ")
                print()
        else:
            print("â„¹ï¸ ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        return len(history)
        
    except Exception as e:
        print(f"âŒ ê¸°ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return 0


async def example_5_export_reports(benchmark_file: str):
    """ì˜ˆì œ 5: ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°"""
    print("\nğŸ“„ ì˜ˆì œ 5: ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°")
    print("=" * 50)
    
    if not benchmark_file:
        print("â„¹ï¸ ë‚´ë³´ë‚¼ ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    formats = ["markdown", "csv", "json"]
    
    try:
        for fmt in formats:
            print(f"ğŸ“ {fmt.upper()} í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” ì¤‘...")
            
            result = await export_benchmark_report(
                benchmark_file=benchmark_file,
                format=fmt
            )
            
            if result.get('message'):
                print(f"  âœ… {result['message']}")
                print(f"     ğŸ“ íŒŒì¼: {result.get('output_file', 'N/A')}")
            else:
                print(f"  âŒ {fmt} ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        return False


async def example_6_real_world_scenario():
    """ì˜ˆì œ 6: ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ ì‹œë‚˜ë¦¬ì˜¤"""
    print("\nğŸŒ ì˜ˆì œ 6: ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ - ìµœì ì˜ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ì°¾ê¸°")
    print("=" * 50)
    
    # ì½”ë”© ê´€ë ¨ í”„ë¡¬í”„íŠ¸
    coding_prompt = """
    ë‹¤ìŒ Python í•¨ìˆ˜ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”:

    def fibonacci(n):
        if n <= 1:
            return n
        return fibonacci(n-1) + fibonacci(n-2)

    ì‹œê°„ ë³µì¡ë„ë¥¼ ê°œì„ í•˜ê³ , ì½”ë“œë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
    """
    
    # ì½”ë”©ì— íŠ¹í™”ëœ ëª¨ë¸ë“¤
    coding_models = [
        "openai/gpt-3.5-turbo",  # ë²”ìš© ëª¨ë¸
        "anthropic/claude-3-haiku",  # ë¹ ë¥¸ ì‘ë‹µ
    ]
    
    try:
        print("ğŸ’» ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        result = await benchmark_models(
            models=coding_models,
            prompt=coding_prompt,
            runs=2,
            delay_seconds=1.0,
            save_results=True
        )
        
        if result.get('results'):
            print("âœ… ì½”ë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
            
            # ì½”ë”© ê´€ë ¨ ë¶„ì„
            print("\nğŸ” ì½”ë”© ëŠ¥ë ¥ ë¶„ì„:")
            
            for model_id, model_result in result['results'].items():
                if model_result.get('success'):
                    metrics = model_result.get('metrics', {})
                    response_preview = model_result.get('response_preview', '')
                    
                    # ì½”ë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                    has_code = '```' in response_preview or 'def ' in response_preview
                    
                    print(f"\nğŸ“Š {model_id}:")
                    print(f"  - ì‘ë‹µ ì‹œê°„: {metrics.get('avg_response_time', 0):.2f}ì´ˆ")
                    print(f"  - ë¹„ìš©: ${metrics.get('avg_cost', 0):.6f}")
                    print(f"  - í’ˆì§ˆ ì ìˆ˜: {metrics.get('quality_score', 0):.1f}")
                    print(f"  - ì½”ë“œ í¬í•¨: {'âœ…' if has_code else 'âŒ'}")
                    print(f"  - ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_preview[:100]}...")
        
        # ì¶”ê°€ ë¶„ì„: ì½”ë”© ì „ìš© ì„±ëŠ¥ ë¹„êµ
        print("\nâš–ï¸ ì½”ë”© ìµœì í™” ê°€ì¤‘ì¹˜ ë¶„ì„:")
        
        coding_weights = {
            "speed": 0.3,      # ë¹ ë¥¸ ê°œë°œì„ ìœ„í•œ ì‘ë‹µ ì†ë„
            "cost": 0.2,       # ë¹„ìš© íš¨ìœ¨ì„±
            "quality": 0.5     # ì½”ë“œ í’ˆì§ˆì´ ê°€ì¥ ì¤‘ìš”
        }
        
        performance_result = await compare_model_performance(
            models=coding_models,
            weights=coding_weights,
            include_cost_analysis=True
        )
        
        if performance_result.get('success'):
            ranking = performance_result.get('ranking', [])
            print(f"ğŸ† ìµœì ì˜ ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸: {ranking[0]['model_id']}")
            print(f"   ì¢…í•© ì ìˆ˜: {ranking[0]['weighted_score']:.3f}")
            
            recommendations = performance_result.get('recommendations', [])
            for rec in recommendations:
                if rec['type'] in ['best_overall', 'highest_quality']:
                    print(f"ğŸ’¡ ì¶”ì²œ: {rec['model']} - {rec['reason']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ‰ OpenRouter MCP Server ë²¤ì¹˜ë§ˆí‚¹ ì˜ˆì œ ì‹œì‘!")
    print("=" * 60)
    
    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("âŒ OPENROUTER_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”:")
        print("   export OPENROUTER_API_KEY='your-api-key-here'")
        return
    
    print(f"âœ… API í‚¤ í™•ì¸ ì™„ë£Œ: {api_key[:8]}...")
    print()
    
    # ì˜ˆì œ ì‹¤í–‰
    examples = [
        ("ê¸°ë³¸ ë²¤ì¹˜ë§ˆí‚¹", example_1_basic_benchmarking),
        ("ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ", example_2_category_comparison),
        ("ê³ ê¸‰ ì„±ëŠ¥ ë¶„ì„", example_3_performance_analysis),
        ("ë²¤ì¹˜ë§ˆí¬ ê¸°ë¡ ì¡°íšŒ", example_4_benchmark_history),
        ("ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€", example_6_real_world_scenario),
    ]
    
    results = {}
    benchmark_file = None
    
    for name, example_func in examples:
        try:
            print(f"\n{'='*20} {name} {'='*20}")
            
            if example_func == example_1_basic_benchmarking:
                # ì²« ë²ˆì§¸ ì˜ˆì œì—ì„œ ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ê²½ë¡œ ë°›ê¸°
                benchmark_file = await example_func()
                results[name] = benchmark_file is not None
            elif example_func == example_4_benchmark_history:
                # ê¸°ë¡ ì¡°íšŒì—ì„œ ê°œìˆ˜ ë°›ê¸°
                count = await example_func()
                results[name] = count > 0
            else:
                # ë‚˜ë¨¸ì§€ ì˜ˆì œë“¤
                result = await example_func()
                results[name] = result
        
        except Exception as e:
            print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            results[name] = False
        
        # ì˜ˆì œ ê°„ ì ì‹œ ëŒ€ê¸°
        await asyncio.sleep(1)
    
    # ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸° ì˜ˆì œ (ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ì´ ìˆì„ ë•Œë§Œ)
    if benchmark_file:
        try:
            print(f"\n{'='*20} ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸° {'='*20}")
            export_result = await example_5_export_reports(benchmark_file)
            results["ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°"] = export_result
        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            results["ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°"] = False
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ¯ ì˜ˆì œ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    successful = sum(1 for success in results.values() if success)
    total = len(results)
    
    for name, success in results.items():
        status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
        print(f"  {name:20} : {status}")
    
    print(f"\nğŸ“Š ì´ {total}ê°œ ì˜ˆì œ ì¤‘ {successful}ê°œ ì„±ê³µ ({successful/total*100:.1f}%)")
    
    if successful > 0:
        print("\nğŸ‰ ë²¤ì¹˜ë§ˆí‚¹ ì˜ˆì œ ì™„ë£Œ!")
        print("ğŸ’¡ Claude Desktopì´ë‚˜ ë‹¤ë¥¸ MCP í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”:")
        print("   - benchmark_models")
        print("   - get_benchmark_history") 
        print("   - compare_model_categories")
        print("   - export_benchmark_report")
        print("   - compare_model_performance")
    else:
        print("\nğŸ˜ ëª¨ë“  ì˜ˆì œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("API í‚¤ì™€ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    # ì˜ˆì œ ì‹¤í–‰
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()